{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "08b39bc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from random import seed\n",
        "\n",
        "import numpy as np\n",
        "from time import time_ns\n",
        "\n",
        "from vulture.utils import to_numpy, do_2D_pca, closest_crop, convert_image\n",
        "from vulture.main import get_dv2_model, get_upsampler_and_expr, get_hr_feats\n",
        "from vulture.feature_prep import get_lr_feats, project\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "DEVICE = \"cuda:1\"\n",
        "\n",
        "SEED = 2\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "907b4343",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/ronan/.cache/torch/hub/ywyue_FiT3D_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[128, 128, 128, 128]\n"
          ]
        }
      ],
      "source": [
        "model_path = \"../trained_models/e5000_full_fit_reg.pth\"\n",
        "cfg_path = \"../vulture/models/configs/upsampler_FU_default.json\"\n",
        "\n",
        "dv2 = get_dv2_model(True, to_half=True, add_flash=True, device=DEVICE)\n",
        "dv2 = dv2.to(DEVICE)\n",
        "\n",
        "upsampler, expr = get_upsampler_and_expr(model_path, cfg_path, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2b920b99",
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = \"fig_data/supp_more_feat_vis\"\n",
        "paths = [f\"{PATH}/{name}.jpg\" for name in ('394', 'sns2_anode_cropped', 'biphase_steel_crop' , 'cells') ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e1943b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def get_lr_hr_feats(img: Image.Image, ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    tr = closest_crop(img.height, img.width, 14, True)\n",
        "    inp_img = (\n",
        "    TF.normalize(\n",
        "        TF.pil_to_tensor(img).to(torch.float32),\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225],\n",
        "    )\n",
        "    .unsqueeze(0)\n",
        "    .to(DEVICE)\n",
        "    )\n",
        "    inp_img_dino = convert_image(img, tr, to_half=True, device_str=DEVICE)\n",
        "    lr_feats, _ = get_lr_feats(dv2, [inp_img_dino], 50, fit3d=True, n_feats_in=expr.n_ch_in)\n",
        "    lr_feats = lr_feats.to(DEVICE)\n",
        "    lr_feats = F.normalize(lr_feats, p=1, dim=1)\n",
        "    with torch.autocast(DEVICE, torch.float16):\n",
        "        hr_feats = upsampler(inp_img, lr_feats)\n",
        "    return (lr_feats, hr_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "afde4409",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale(a: np.ndarray) -> np.ndarray:\n",
        "    a_min = a.min(axis=(0, 1), keepdims=True)\n",
        "    a_max = a.max(axis=(0, 1), keepdims=True)\n",
        "    out = (a - a_min) / (a_max - a_min)\n",
        "    return out\n",
        "\n",
        "def truncate_and_rescale(x: torch.Tensor) -> np.ndarray:\n",
        "    x_np = x.cpu()[0].numpy().astype(np.float32)\n",
        "    x_red = x_np.transpose((1, 2, 0))[:, :, 0:3]\n",
        "    x_rescaled = rescale(x_red)\n",
        "    return x_rescaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "05e7606b",
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs, lr_feats, hr_feats = [], [], []\n",
        "for img_path in paths:\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    lr, hr = get_lr_hr_feats(img)\n",
        "\n",
        "    imgs.append(img)\n",
        "    lr_feats.append(truncate_and_rescale(lr))\n",
        "    hr_feats.append(truncate_and_rescale(hr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "18227e3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def center_crop(img, crop_h, crop_w):\n",
        "    \"\"\"\n",
        "    Center crop a PIL Image or numpy array to (crop_h, crop_w).\n",
        "    \"\"\"\n",
        "    if isinstance(img, Image.Image):\n",
        "        w, h = img.size\n",
        "        left = (w - crop_w) // 2\n",
        "        top = (h - crop_h) // 2\n",
        "        right = left + crop_w\n",
        "        bottom = top + crop_h\n",
        "        return img.crop((left, top, right, bottom))\n",
        "    elif isinstance(img, np.ndarray):\n",
        "        h, w = img.shape[:2]\n",
        "        top = (h - crop_h) // 2\n",
        "        left = (w - crop_w) // 2\n",
        "        return img[top:top+crop_h, left:left+crop_w, ...]\n",
        "    else:\n",
        "        raise TypeError(\"Input must be a PIL.Image.Image or numpy.ndarray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c5a26158",
      "metadata": {},
      "outputs": [],
      "source": [
        "def center_crop_aspect(img, aspect_ratio):\n",
        "    \"\"\"\n",
        "    Center crop a PIL Image or numpy array to the given aspect ratio.\n",
        "\n",
        "    Parameters:\n",
        "        img: PIL.Image.Image or np.ndarray\n",
        "        aspect_ratio: float (width / height)\n",
        "\n",
        "    Returns:\n",
        "        Cropped image (same type as input)\n",
        "    \"\"\"\n",
        "    if isinstance(img, Image.Image):\n",
        "        w, h = img.size\n",
        "    elif isinstance(img, np.ndarray):\n",
        "        h, w = img.shape[:2]\n",
        "    else:\n",
        "        raise TypeError(\"Input must be a PIL.Image.Image or numpy.ndarray\")\n",
        "\n",
        "    current_ar = w / h\n",
        "\n",
        "    if current_ar > aspect_ratio:\n",
        "        # Image is too wide, crop width\n",
        "        new_w = int(h * aspect_ratio)\n",
        "        new_h = h\n",
        "    else:\n",
        "        # Image is too tall, crop height\n",
        "        new_w = w\n",
        "        new_h = int(w / aspect_ratio)\n",
        "\n",
        "    left = (w - new_w) // 2\n",
        "    top = (h - new_h) // 2\n",
        "\n",
        "    if isinstance(img, Image.Image):\n",
        "        return img.crop((left, top, left + new_w, top + new_h))\n",
        "    else:\n",
        "        return img[top:top + new_h, left:left + new_w, ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f0d059c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "fig, axs = plt.subplots(nrows=len(imgs), ncols=3, figsize=(18, 12))\n",
        "\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "TITLE_FS = 25\n",
        "LABEL_FS = 23\n",
        "TICK_FS = 21\n",
        "\n",
        "titles = ('Image', 'Low-res DINOv2', 'HR ViT')\n",
        "img_names = ('Iron alloy', 'SnS2 anode', 'Biphase steel', 'Plant cells')\n",
        "h, w, _ = hr_feats[0].shape\n",
        "for i, (img, lr, hr) in enumerate(zip(imgs, lr_feats, hr_feats)):\n",
        "    axs[i, 0].imshow(center_crop_aspect(img, w / h))\n",
        "    axs[i, 1].imshow(center_crop_aspect(lr, (w // 14) / (h // 14) ))\n",
        "    axs[i, 2].imshow(center_crop_aspect(hr, w / h))\n",
        "\n",
        "    axs[i, 0].set_ylabel(img_names[i], fontsize=LABEL_FS)\n",
        "\n",
        "    for j, ax in enumerate(axs[i]):\n",
        "        # ax.set_axis_off()\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_frame_on(False)\n",
        "        if i == 0:\n",
        "            ax.set_title(titles[j], fontsize=TITLE_FS)\n",
        "plt.tight_layout()\n",
        "plt.savefig('fig_out/supp_feat_vis.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
