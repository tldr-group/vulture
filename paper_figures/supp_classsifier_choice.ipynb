{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9642ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N CPUS: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronan/miniconda3/envs/dv2/lib/python3.12/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from interactive_seg_backend.file_handling import load_image, load_labels\n",
    "from interactive_seg_backend.main import featurise, train_and_apply, TrainingConfig, FeatureConfig\n",
    "\n",
    "from yoeo.main import (\n",
    "    get_hr_feats,\n",
    "    get_dv2_model,\n",
    "    get_upsampler_and_expr,\n",
    ")\n",
    "from yoeo.utils import to_numpy\n",
    "from is_helpers import get_deep_feats, train_model_over_images, apply_model_over_images, eval_preds\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.color import label2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Any, Literal\n",
    "\n",
    "SEED = 10673\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e114076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/ywyue_FiT3D_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "dv2 = get_dv2_model(True, device=DEVICE)\n",
    "\n",
    "model_path = \"../trained_models/e5000_full_fit_reg.pth\"\n",
    "cfg_path = \"../yoeo/models/configs/combined_no_shift.json\"\n",
    "\n",
    "upsampler, expr = get_upsampler_and_expr(model_path, cfg_path, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af40e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"fig_data/supp_classifier_choice\"\n",
    "img = load_image(f\"{PATH}/img.tif\")\n",
    "labels = load_labels(f\"{PATH}/labels.tif\")\n",
    "ground_truth = load_labels(f\"{PATH}/gt.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669d9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = (\"Classical\", \"+HR ViT\")\n",
    "classifiers = (\"linear_regression\", \"logistic_regression\", \"random_forest\", \"xgb\", 'mlp')\n",
    "results: dict[str, Any] = {\"Classical\": {}, \"+HR ViT\": {}}\n",
    "\n",
    "params = [{}, {}, {'class_weight': 'balanced'}, {'class_weight': 'balanced'}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959d452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tc = TrainingConfig(FeatureConfig())\n",
    "classical_feats = featurise(img, dummy_tc)\n",
    "results[\"Classical\"]['features'] = classical_feats\n",
    "\n",
    "deep_feats = get_deep_feats(img, dv2, upsampler, expr, 32)\n",
    "results[\"+HR ViT\"]['features'] = np.concatenate((classical_feats, deep_feats), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b79aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for featureset in featuresets:\n",
    "    for classifier in classifiers:\n",
    "        feats = results[featureset][\"features\"]\n",
    "        tc = TrainingConfig(FeatureConfig(), classifier)\n",
    "        seg, _, _ = train_and_apply(feats, labels, tc)\n",
    "        results[featureset][classifier] = seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb4c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_list = [[255, 255, 255], [31, 119, 180], [255, 127, 14], [44, 160, 44], [255, 0, 0]]\n",
    "color_list = [[255, 255, 255], [0, 62, 131], [181, 209, 204], [250, 43, 0], [255, 184, 82]]\n",
    "COLORS = np.array(color_list) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d10cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_labels_as_overlay(labels: np.ndarray, img: Image.Image, colors: list, alpha: float=1.0) -> Image.Image:\n",
    "    labels_unsqueezed = np.expand_dims(labels, -1)\n",
    "\n",
    "    overlay = label2rgb(labels, colors=colors[1:], kind='overlay', bg_label=0, image_alpha=1, alpha=alpha)\n",
    "    out = np.where(labels_unsqueezed, overlay * 255, np.array(img)).astype(np.uint8)\n",
    "    img_with_labels = Image.fromarray(out)\n",
    "    return img_with_labels\n",
    "\n",
    "def add_inset_zoom(xywh: list[int], fig_xywh: list[float], img_arr: np.ndarray, labels: np.ndarray | None, ax ) -> object:\n",
    "    x0, y0, w, h = xywh\n",
    "    H, W, C = img_arr.shape\n",
    "    inset_data = np.zeros_like(img_arr)\n",
    "    inset_data[y0:y0+h, x0:x0+w, :] = img_arr[y0:y0+h, x0:x0+w, :]\n",
    "\n",
    "    axin = ax.inset_axes(\n",
    "        fig_xywh, xlim=(x0, x0+w), ylim=(y0, y0+h))\n",
    "    axin.set_xticks([])\n",
    "    axin.set_yticks([])\n",
    "    #axin.set_axis_off()\n",
    "    if labels is not None:\n",
    "        inset_data = label2rgb(labels, img_arr, COLORS[1:], kind='overlay', alpha=1, bg_label=-1)\n",
    "        axin.imshow(inset_data,)\n",
    "    else:\n",
    "        axin.imshow(inset_data, cmap=\"binary_r\",) # cmap=\"binary_r\"\n",
    "    ax.indicate_inset_zoom(axin, edgecolor=\"black\", lw=2)\n",
    "    axin.set_ylim((y0 + h, y0))\n",
    "\n",
    "    axin.patch.set_edgecolor('black')  \n",
    "\n",
    "    axin.patch.set_linewidth(4)  \n",
    "\n",
    "    return axin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb7a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "n_examples = 3\n",
    "n_cols = len(classifiers)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "TITLE_FS = 25\n",
    "LABEL_FS = 23\n",
    "TICK_FS = 21\n",
    "\n",
    "width = 4\n",
    "fig, axs = plt.subplots(nrows=n_examples, ncols=n_cols, figsize=(width * n_cols, width * n_examples))\n",
    "\n",
    "for row in axs:\n",
    "    for ax in row:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_frame_on(False)\n",
    "\n",
    "img_with_labels = apply_labels_as_overlay(labels[0], Image.fromarray(img).convert('RGB'), COLORS)\n",
    "axs[0, 1].set_title('Image + labels', fontsize=TITLE_FS)\n",
    "axs[0, 1].imshow(img_with_labels)\n",
    "axs[0, 2].set_title('Ground truth', fontsize=TITLE_FS)\n",
    "axs[0, 2].imshow(label2rgb(ground_truth + 1, colors=COLORS[1:]))\n",
    "\n",
    "titles = (\"Linear\", \"Logistic regression\", \"Random Forest\", \"XGB\", \"MLP\")\n",
    "for i, (featureset, sub_dict) in enumerate(results.items()):\n",
    "    print(featureset)\n",
    "    for j, classifier in enumerate(classifiers):\n",
    "        pred = sub_dict[classifier]\n",
    "        axs[i + 1, j].imshow(label2rgb(pred + 1, colors=COLORS[1:]))\n",
    "        add_inset_zoom([40, 80, 160, 100], [0.6, 0.1, 0.5, 0.4], np.array(img_with_labels), pred, axs[i + 1, j])\n",
    "\n",
    "        if i == 0:\n",
    "            axs[i + 1, j].set_title(titles[j], fontsize=TITLE_FS)\n",
    "        if j == 0: \n",
    "            axs[i + 1, j].set_ylabel(featureset, fontsize=TITLE_FS)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'fig_out/supp_classifier_choice.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c9ce7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "[00/23] - 000.tif\n",
      "[10/23] - 010.tif\n",
      "[20/23] - 020.tif\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "[00/23] - 000.tif\n",
      "[10/23] - 010.tif\n",
      "[20/23] - 020.tif\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "[00/23] - 000.tif\n",
      "[10/23] - 010.tif\n",
      "[20/23] - 020.tif\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "[00/23] - 000.tif\n",
      "[10/23] - 010.tif\n",
      "[20/23] - 020.tif\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "Finished featurising\n",
      "[00/23] - 000.tif\n",
      "[10/23] - 010.tif\n",
      "[20/23] - 020.tif\n"
     ]
    }
   ],
   "source": [
    "PATH = \"fig_data/is_benchmark\"\n",
    "AllowedDatasets = Literal[\"Ni_superalloy_SEM\", \"T_cell_TEM\", \"Cu_ore_RLM\"]\n",
    "\n",
    "TRAIN_IMG_FNAMES: dict[AllowedDatasets, list[str]] = {\"Cu_ore_RLM\": [\"004\", \"028\", \"049\", \"077\"], \n",
    "                                                      \"Ni_superalloy_SEM\": [\"000\", \"001\", \"005\", \"007\"], \n",
    "                                                      \"T_cell_TEM\": [\"000\", \"005\", \"007\", \"026\"]\n",
    "                                                      }\n",
    "\n",
    "all_classical_preds: dict[str, dict[str, np.ndarray]] = {k: {} for k in classifiers}\n",
    "all_deep_preds: dict[str, dict[str, np.ndarray]] = {k: {} for k in classifiers}\n",
    "\n",
    "chosen_dataset = \"Ni_superalloy_SEM\"\n",
    "for classifier, params in zip(classifiers, params):\n",
    "    feat_cfg = FeatureConfig()\n",
    "\n",
    "    classical_train_cfg = TrainingConfig(feat_cfg, n_samples=-1, add_dino_features=False, classifier=classifier, classifier_params = params)\n",
    "    classical_model, _ = train_model_over_images(chosen_dataset, classical_train_cfg, PATH, TRAIN_IMG_FNAMES[chosen_dataset], dv2, upsampler, expr)\n",
    "\n",
    "    deep_train_cfg = TrainingConfig(feat_cfg, n_samples=-1, add_dino_features=True, classifier=classifier, classifier_params = params)\n",
    "    deep_model, pca = train_model_over_images(chosen_dataset, deep_train_cfg, PATH, TRAIN_IMG_FNAMES[chosen_dataset], dv2, upsampler, expr)\n",
    "\n",
    "    classical_preds = apply_model_over_images(chosen_dataset, classical_train_cfg, classical_model, PATH, dv2, upsampler, expr, verbose=False)\n",
    "    deep_preds = apply_model_over_images(chosen_dataset, deep_train_cfg, deep_model, PATH, dv2, upsampler, expr, verbose=True, existing_pca=pca)\n",
    "\n",
    "    all_classical_preds[classifier] = classical_preds\n",
    "    all_deep_preds[classifier] = deep_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c9d1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== linear_regression ========\n",
      "mIoU_classical: 0.4071+/-0.0768 vs mIoU_deep: 0.6259+/-0.1486\n",
      "\n",
      "======== logistic_regression ========\n",
      "mIoU_classical: 0.3842+/-0.0652 vs mIoU_deep: 0.5864+/-0.0518\n",
      "\n",
      "======== random_forest ========\n",
      "mIoU_classical: 0.5322+/-0.1401 vs mIoU_deep: 0.6925+/-0.1142\n",
      "\n",
      "======== xgb ========\n",
      "mIoU_classical: 0.5403+/-0.1249 vs mIoU_deep: 0.6803+/-0.1240\n",
      "\n",
      "======== mlp ========\n",
      "mIoU_classical: 0.5238+/-0.1147 vs mIoU_deep: 0.6341+/-0.0936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    classical_preds, deep_preds = all_classical_preds[classifier], all_deep_preds[classifier]\n",
    "    miou_classical, miou_std_classical = eval_preds(chosen_dataset, classical_preds, PATH)\n",
    "    miou_deep, miou_std_deep = eval_preds(chosen_dataset, deep_preds, PATH )\n",
    "    print(f\"======== {classifier} ========\")\n",
    "    print(f\"mIoU_classical: {miou_classical:.4f}+/-{miou_std_classical:.4f} vs mIoU_deep: {miou_deep:.4f}+/-{miou_std_deep:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
