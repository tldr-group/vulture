{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8212341b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from random import seed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import label2rgb\n",
        "from tifffile import imread\n",
        "\n",
        "from vulture.main import get_dv2_model, get_upsampler_and_expr, get_hr_feats\n",
        "from vulture.utils import to_numpy, do_2D_pca, closest_crop, convert_image\n",
        "from vulture.feature_prep import get_lr_feats\n",
        "\n",
        "from interactive_seg_backend.file_handling import load_labels\n",
        "\n",
        "from types import NoneType\n",
        "\n",
        "\n",
        "SEED = 10672\n",
        "seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = \"cuda:0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "533e93cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "color_list = [[255, 255, 255], [0, 62, 131], [181, 209, 204], [250, 43, 0], [255, 184, 82]]\n",
        "COLORS = np.array(color_list) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0b44cd38",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_labels_as_overlay(labels: np.ndarray, img: Image.Image, colors: list, alpha: float=1.0) -> Image.Image:\n",
        "    labels_unsqueezed = np.expand_dims(labels, -1)\n",
        "\n",
        "    overlay = label2rgb(labels, colors=colors[1:], kind='overlay', bg_label=0, image_alpha=1, alpha=alpha)\n",
        "    out = np.where(labels_unsqueezed, overlay * 255, np.array(img)).astype(np.uint8)\n",
        "    img_with_labels = Image.fromarray(out)\n",
        "    return img_with_labels\n",
        "\n",
        "\n",
        "def rescale(a: np.ndarray) -> np.ndarray:\n",
        "    a_min = a.min(axis=(0, 1), keepdims=True)\n",
        "    a_max = a.max(axis=(0, 1), keepdims=True)\n",
        "    out = (a - a_min) / (a_max - a_min)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "03b2fe4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"../trained_models/e5000_full_fit_reg.pth\"\n",
        "cfg_path = \"../vulture/models/configs/combined_no_shift.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6591f41",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/ronan/.cache/torch/hub/ywyue_FiT3D_main\n",
            "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_small_patch14_reg4_dinov2.lvd142m)\n",
            "INFO:timm.models._hub:[timm/vit_small_patch14_reg4_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[128, 128, 128, 128]\n"
          ]
        }
      ],
      "source": [
        "dv2 = get_dv2_model(True, device=DEVICE)\n",
        "\n",
        "model_path = \"../trained_models/fit_reg_f128.pth\"\n",
        "# cfg_path = \"../vulture/models/configs/combined_no_shift.json\"\n",
        "\n",
        "upsampler, expr = get_upsampler_and_expr(model_path, None, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d58ed6f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = \"fig_data/natural\"\n",
        "\n",
        "img_names = ['plant', 'bird', 'church', 'balls']\n",
        "imgs: list[Image.Image] = []\n",
        "lr_feat_vis: list[np.ndarray] = []\n",
        "hr_feat_vis: list[np.ndarray] = []\n",
        "classical_preds: list[np.ndarray] = []\n",
        "deep_preds: list[np.ndarray] = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for name in img_names:\n",
        "        img_path = f\"{DATA_PATH}/{name}.png\"\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        tr = closest_crop(img.height, img.width, 14, True)\n",
        "\n",
        "\n",
        "        inp_img = (\n",
        "            TF.normalize(\n",
        "                TF.pil_to_tensor(img).to(torch.float32),\n",
        "                [0.485, 0.456, 0.406],\n",
        "                [0.229, 0.224, 0.225],\n",
        "            )\n",
        "            .unsqueeze(0)\n",
        "            .to(DEVICE)\n",
        "        )\n",
        "        inp_img_dino = convert_image(img, tr, to_half=True, device_str=DEVICE)\n",
        "\n",
        "\n",
        "        labels = load_labels(f\"{DATA_PATH}/{name}_labels.tiff\")\n",
        "        with_labels = apply_labels_as_overlay(labels, img, COLORS)\n",
        "\n",
        "        imgs.append(with_labels)\n",
        "\n",
        "        lr_feats, _ = get_lr_feats(dv2, [inp_img_dino], 50, fit3d=True, n_feats_in=expr.n_ch_in)\n",
        "\n",
        "        lr_feats = F.normalize(lr_feats, p=1, dim=1)\n",
        "        with torch.autocast(DEVICE, torch.float16):\n",
        "            hr_feats = upsampler(inp_img, lr_feats)\n",
        "        \n",
        "        lr_feats_np = lr_feats.cpu()[0].numpy().astype(np.float32)\n",
        "        hr_feats_np = hr_feats.cpu()[0].numpy().astype(np.float32)\n",
        "\n",
        "        lr_feats_red = lr_feats_np.transpose((1, 2, 0))[:, :, 0:3]\n",
        "        lr_feats_red = rescale(lr_feats_red)\n",
        "        hr_feats_red = hr_feats_np.transpose((1, 2, 0))[:, :, 0:3]\n",
        "        hr_feats_red = rescale(hr_feats_red)\n",
        "\n",
        "        lr_feat_vis.append(lr_feats_red)\n",
        "        hr_feat_vis.append(hr_feats_red)\n",
        "\n",
        "        classical_pred =  load_labels(f\"{DATA_PATH}/{name}_seg_classical.tiff\") + 1\n",
        "        classical_preds.append(label2rgb(classical_pred, None, COLORS[1:]))\n",
        "\n",
        "        deep_pred =  load_labels(f\"{DATA_PATH}/{name}_seg_deep.tiff\") + 1\n",
        "        deep_preds.append(label2rgb(deep_pred, None, COLORS[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0a3cbeb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "TITLE_FS = 25\n",
        "N_ROWS = len(img_names) \n",
        "N_COLS = 5\n",
        "FIG_W = 3\n",
        "\n",
        "titles = [\"Image + labels\", \"LR features\", \"HR ViT\", \"Classical seg.\", \"+HR ViT seg.\"]\n",
        "fig, axs = plt.subplots(nrows=N_ROWS, ncols=N_COLS, figsize=(N_COLS * FIG_W, N_ROWS * FIG_W))\n",
        "for i in range(N_ROWS):\n",
        "\n",
        "    arrs = (imgs[i], lr_feat_vis[i], hr_feat_vis[i], classical_preds[i], deep_preds[i])\n",
        "\n",
        "    for j, arr in enumerate(arrs):\n",
        "        ax = axs[i,j]\n",
        "        ax.set_axis_off()\n",
        "        ax.imshow(arr)\n",
        "\n",
        "        if i == 0:\n",
        "            ax.set_title(titles[j], fontsize=TITLE_FS)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('fig_out/supp_natural_examples.png' ,bbox_inches='tight')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
