{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb0a91cd210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from time import time_ns\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "from yoeo.main import get_dv2_model, get_upsampler_and_expr, get_hr_feats\n",
    "from yoeo.utils import load_image, convert_image, to_numpy, closest_crop, Experiment, do_2D_pca, add_flash_attention\n",
    "\n",
    "from yoeo.comparisons.lift import ViTLiFTExtractor\n",
    "from yoeo.comparisons.strided import StridedDv2\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "SEED = 10672\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mem_time(\n",
    "    device: torch.device | str,\n",
    "    featurise_fn: Callable\n",
    ") -> tuple[float, float]:\n",
    "    torch.cuda.reset_peak_memory_stats(device)  # s.t memory is accurate\n",
    "    torch.cuda.synchronize(device)  # s.t time is accurate\n",
    "\n",
    "    def _to_MB(x: int) -> float:\n",
    "        return x / (1024**2)\n",
    "\n",
    "    def _to_s(t: int) -> float:\n",
    "        return t / 1e9\n",
    "\n",
    "    start_m = torch.cuda.max_memory_allocated(device)\n",
    "    start_t = time_ns()\n",
    "\n",
    "    featurise_fn()\n",
    "\n",
    "    end_m = torch.cuda.max_memory_allocated(device)\n",
    "    torch.cuda.synchronize(device)\n",
    "    end_t = time_ns()\n",
    "\n",
    "    return _to_MB(end_m - start_m), _to_s(end_t - start_t)\n",
    "\n",
    "def rescale(arr: np.ndarray, swap_channels: bool=True) -> np.ndarray:\n",
    "    if swap_channels:\n",
    "        arr = np.transpose(arr, (1, 2, 0))\n",
    "    h, w, c = arr.shape\n",
    "    flat = arr.reshape((h * w, c))\n",
    "    rescaled_flat = MinMaxScaler(clip=True).fit_transform(flat)\n",
    "    return rescaled_flat.reshape((h, w, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/ywyue_FiT3D_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "normal_dv2 = get_dv2_model(fit_3d=False, device=DEVICE)\n",
    "dv2 = get_dv2_model(True, device=DEVICE)\n",
    "\n",
    "model_path = \"../trained_models/e5000_full_fit_reg.pth\"\n",
    "cfg_path = \"../yoeo/models/configs/combined_no_shift.json\"\n",
    "\n",
    "upsampler, expr = get_upsampler_and_expr(model_path, cfg_path, device=DEVICE)\n",
    "# upsampler = upsampler.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "featup_jbu = torch.hub.load(\"mhamilton723/FeatUp\", \"dinov2\", use_norm=True).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "strided = StridedDv2('dinov2', 'vits14_reg', 1).to(DEVICE).eval()\n",
    "strided.model = add_flash_attention(strided.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Backbone: dino_vits8\n",
      "Loaded LiFT module from: ../trained_models/lift/lift_dino_vits8.pth\n"
     ]
    }
   ],
   "source": [
    "lift_path = \"../trained_models/lift/lift_dino_vits8.pth\"\n",
    "lift = ViTLiFTExtractor('dino_vits8', lift_path=lift_path, channel=384, facet='key')\n",
    "# lift.extractor.model = add_flash_attention(lift.extractor.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/andrehuang_loftup_main\n"
     ]
    }
   ],
   "source": [
    "torch_hub_name = \"loftup_dinov2s_reg\"\n",
    "lu_upsampler = torch.hub.load(\"andrehuang/loftup\", torch_hub_name, pretrained=True)\n",
    "lu_upsampler = lu_upsampler.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"fig_data/perf_landscape/church_compare.png\"\n",
    "img = Image.open(PATH).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _our_featurise(img: Image.Image, dv2: torch.nn.Module, upsampler: torch.nn.Module, expr: Experiment) -> np.ndarray:\n",
    "    hr_feats = get_hr_feats(img, dv2, upsampler, DEVICE, n_ch_in=expr.n_ch_in)\n",
    "    hr_feats_np = to_numpy(hr_feats)\n",
    "    reduced_hr = hr_feats_np[:3]\n",
    "    return reduced_hr\n",
    "\n",
    "@torch.no_grad()\n",
    "def _original_featurise(img: Image.Image, dv2: torch.nn.Module) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr)\n",
    "    _, _, h, w = tensor.shape\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        dino_feats = dv2.forward_features(tensor)['x_norm_patchtokens']\n",
    "    n_patch_w, n_patch_h = w // 14, h // 14\n",
    "    dino_feats = dino_feats.permute((0, 2, 1))\n",
    "    dino_feats = dino_feats.reshape((1, -1, n_patch_h, n_patch_w,))\n",
    "    dino_feats_np = to_numpy(dino_feats)\n",
    "    return dino_feats_np\n",
    "\n",
    "@torch.no_grad()\n",
    "def _jbu_featurise(img: Image.Image, jbu: torch.nn.Module) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        jbu_feats = jbu(tensor.to(torch.float32))\n",
    "    jbu_feats = F.interpolate(jbu_feats, (_h, _w))\n",
    "    jbu_feats_np = to_numpy(jbu_feats)\n",
    "    return jbu_feats_np[:3]\n",
    "\n",
    "@torch.no_grad()\n",
    "def _lift_featurise(path: str, img: Image.Image, lift: ViTLiFTExtractor, n: int=3, patch_size: int=8) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "\n",
    "    image_batch, _ = lift.preprocess(path, (_h, _w))\n",
    "    image_batch = image_batch.to(DEVICE)\n",
    "\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        lift_feats = lift.extract_descriptors_iterative_lift(image_batch, lift_iter=n)\n",
    "    _, _, c = lift_feats.shape\n",
    "    sf = int(patch_size / (2**n))\n",
    "    reshaped = lift_feats.squeeze(0).T.reshape((c, _h // sf, _w //  sf))\n",
    "    return to_numpy(reshaped)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _strided_featurise(img: Image.Image, strided: StridedDv2) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        strided_feats = strided(tensor)\n",
    "    strided_feats = F.interpolate(strided_feats, (_h, _w))\n",
    "    strided_feats_feats_np = to_numpy(strided_feats)\n",
    "    return strided_feats_feats_np\n",
    "\n",
    "@torch.no_grad()\n",
    "def _loftup_featurise(img: Image.Image, dv2: torch.nn.Module, upsampler: torch.nn.Module) -> np.ndarray:\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "    tensor = convert_image(img, tr)\n",
    "    _, _, h, w = tensor.shape\n",
    "    lr_feats: torch.Tensor = dv2.forward_features(tensor)['x_norm_patchtokens']\n",
    "    n_patch_w, n_patch_h = w // 14, h // 14\n",
    "    lr_feats = lr_feats.permute((0, 2, 1))\n",
    "    lr_feats = lr_feats.reshape((1, -1, n_patch_h, n_patch_w,))\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        hr_feats = lu_upsampler(lr_feats, tensor) \n",
    "    \n",
    "    feats = F.interpolate(hr_feats, (_h, _w))\n",
    "\n",
    "    feats = to_numpy(feats)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_featurise = partial(_our_featurise, img=img, dv2=dv2, upsampler=upsampler, expr=expr)\n",
    "original_featurise = partial(_original_featurise, img=img, dv2=normal_dv2)\n",
    "jbu_featurise = partial(_jbu_featurise, img=img, jbu=featup_jbu)\n",
    "lift_featurise = partial(_lift_featurise, path=PATH, img=img, lift=lift)\n",
    "strided_featurise = partial(_strided_featurise, img=img, strided=strided)\n",
    "loftup_featurise = partial(_loftup_featurise, img=img, dv2=normal_dv2, upsampler=upsampler)\n",
    "\n",
    "fns = (our_featurise, jbu_featurise, lift_featurise, strided_featurise, loftup_featurise)\n",
    "featuriser_names = (\"Ours\", \"FeatUp (JBU)\", \"LiFT\", \"Strided\", 'LoftUp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours\n",
      "FeatUp (JBU)\n",
      "LiFT\n",
      "Strided\n",
      "LoftUp\n"
     ]
    }
   ],
   "source": [
    "# dry run\n",
    "for i, fn in enumerate(fns):\n",
    "    print(featuriser_names[i])\n",
    "    mem, time = measure_mem_time(DEVICE, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ours': {'time': 0.10608810800000001, 'mem': 293.193359375}, 'FeatUp (JBU)': {'time': 0.14406022200000002, 'mem': 1039.6162109375}, 'LiFT': {'time': 0.0716691202, 'mem': 346.13818359375}, 'Strided': {'time': 2.2429712376, 'mem': 1113.39892578125}, 'LoftUp': {'time': 0.15695184199999998, 'mem': 2107.9921875}}\n"
     ]
    }
   ],
   "source": [
    "mem_time_results: dict[str, dict] = {}\n",
    "N_REPEATS =5\n",
    "for name, fn in zip(featuriser_names, fns):\n",
    "    mems, times = [], []\n",
    "    for i in range(N_REPEATS):\n",
    "        mem, time = measure_mem_time(DEVICE, fn)\n",
    "        # if name in ('Strided', 'LiFT'):\n",
    "        #     # LiFT & strided upsample all 384 DINO features, JBU & Ours upsample to 128\n",
    "        #     # for fair comparison we scale memory of those approaches with that ratio\n",
    "        #     mem *= 128 / 384\n",
    "        mems.append(mem)\n",
    "        times.append(time)\n",
    "    mem_time_results[name] = {'time': np.mean(times), 'mem': np.mean(mems)}\n",
    "print(mem_time_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours: 224\n",
      "Ours: 336\n",
      "Ours: 448\n",
      "Ours: 560\n",
      "Ours: 672\n",
      "Ours: 784\n",
      "FeatUp (JBU): 224\n",
      "FeatUp (JBU): 336\n",
      "FeatUp (JBU): 448\n",
      "FeatUp (JBU): 560\n",
      "FeatUp (JBU): 672\n",
      "FeatUp (JBU): 784\n",
      "LiFT: 224\n",
      "LiFT: 336\n",
      "LiFT: 448\n",
      "LiFT: 560\n",
      "LiFT: 672\n",
      "LiFT: 784\n",
      "Strided: 224\n",
      "Strided: 336\n",
      "Strided: 448\n",
      "Strided: 560\n",
      "Strided: 672\n",
      "Strided: 784\n",
      "LoftUp: 224\n",
      "LoftUp: 336\n",
      "LoftUp: 448\n",
      "LoftUp: 560\n",
      "LoftUp: 672\n",
      "LoftUp: 784\n",
      "{'Ours': [130.71044921875, 290.13134765625, 513.94873046875, 802.20849609375, 1154.8046875, 1569.126953125], 'FeatUp (JBU)': [483.87939453125, 1036.48681640625, 1805.619140625, 2793.9111328125, 4002.5068359375, 5429.61474609375], 'LiFT': [176.0341796875, 346.08349609375, 577.62109375, 891.05322265625, 1787.07373046875, 3256.2421875], 'Strided': [499.53759765625, 1113.162109375, 1984.15673828125, 3113.04736328125, 4499.18798828125, 6141.45361328125], 'LoftUp': [572.8203125, 2108.99560546875, 5803.9755859375, 13198.03515625]}\n",
      "{'Ours': [0.086826356, 0.106560184, 0.180681802, 0.270776957, 0.41630782, 0.576471766], 'FeatUp (JBU)': [0.062742248, 0.148263699, 0.256661243, 0.39988282, 0.562508536, 0.756910906], 'LiFT': [0.035706072, 0.06204121, 0.142282181, 0.233073095, 0.362725374, 0.538000097], 'Strided': [0.439804164, 2.239042675, 7.595817886, 19.697671309, 42.877516195, 80.809760935], 'LoftUp': [0.073938471, 0.156068074, 0.31286518, 0.627324554]}\n"
     ]
    }
   ],
   "source": [
    "# lengths = list(range(224, 1344 - 64, 64))\n",
    "lengths = list(range(224, 800, 112))\n",
    "\n",
    "our_featurise_l = partial(_our_featurise, dv2=dv2, upsampler=upsampler, expr=expr)\n",
    "jbu_featurise_l = partial(_jbu_featurise, jbu=featup_jbu)\n",
    "lift_featurise_l = partial(_lift_featurise, path=PATH, lift=lift)\n",
    "strided_featurise_l = partial(_strided_featurise, strided=strided)\n",
    "loftup_featurise_l = partial(_loftup_featurise, dv2=normal_dv2, upsampler=upsampler)\n",
    "\n",
    "fns_vs_l = (our_featurise_l, jbu_featurise_l, lift_featurise_l, strided_featurise_l, loftup_featurise_l)\n",
    "\n",
    "mem_vs_l: dict[str, list[float]] = {}\n",
    "time_vs_l: dict[str, list[float]] = {}\n",
    "\n",
    "for name, fn in zip(featuriser_names, fns_vs_l):\n",
    "    mems, times = [], []\n",
    "    for l in lengths:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"{name}: {l}\")\n",
    "        resized = img.resize((l, l))\n",
    "        if name  == 'LoftUp' and l > 670:\n",
    "            continue\n",
    "        if name in ('FeatUp (JBU)', 'Strided') and l > 800:\n",
    "            continue\n",
    "        bound = partial(fn, img=resized)\n",
    "        mem, time = measure_mem_time(DEVICE, bound)\n",
    "        \n",
    "        # if name in ('Strided', 'LiFT'):\n",
    "\n",
    "        #     mem *= 128 / 384\n",
    "        mems.append(mem)\n",
    "        times.append(time)\n",
    "    mem_vs_l[name] = mems\n",
    "    time_vs_l[name] = times\n",
    "\n",
    "print(mem_vs_l)\n",
    "print(time_vs_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 24, 3)\n"
     ]
    }
   ],
   "source": [
    "original_pcaed = do_2D_pca(original_featurise(), 3)\n",
    "im0 = rescale(original_pcaed, swap_channels=False)\n",
    "print(im0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = rescale(our_featurise())\n",
    "im2 = rescale(jbu_featurise())\n",
    "lift_feats = lift_featurise()\n",
    "pcaed = do_2D_pca(lift_feats, 3)\n",
    "im3 = rescale(pcaed, swap_channels=False).astype(np.float32)\n",
    "stride_feats = strided_featurise()\n",
    "strided_pcaed = do_2D_pca(stride_feats, 3)\n",
    "im4 = rescale(strided_pcaed, swap_channels=False)\n",
    "loft_feats = loftup_featurise()\n",
    "loft_pca = do_2D_pca(loft_feats, 3)\n",
    "im5 = rescale(loft_pca, swap_channels=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "imgs = (im1, im2, im3, im4, im5)\n",
    "fig, axs = plt.subplots(ncols=len(imgs))\n",
    "fig.set_size_inches((16, 10))\n",
    "for i in range(len(imgs)):\n",
    "    axs[i].imshow(imgs[i])\n",
    "    axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 2, 1], height_ratios=[1, 1])  \n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))  \n",
    "# Create subplots in specific grid locations\n",
    "ax1 = fig.add_subplot(gs[0, 0])  # Top-left\n",
    "ax2 = fig.add_subplot(gs[1, 0])  # Bottom-left\n",
    "ax3 = fig.add_subplot(gs[:, 1])  # Middle column (spanning both rows)\n",
    "ax4 = fig.add_subplot(gs[0, 2])  # Top-right\n",
    "ax5 = fig.add_subplot(gs[1, 2])  # Bottom-right\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# inset_locs = ((0.215, 0.05, 0.3, 0.3), (0.275, 0.82, 0.3, 0.3),  (0.25, 0.445, 0.3, 0.3), (0.80, 0.74, 0.3, 0.3), (0.80, 0.74, 0.3, 0.3))\n",
    "inset_locs = ((0.215, 0.05, 0.3, 0.3), (0.10, 0.60, 0.3, 0.3),  (0.1, 0.3, 0.3, 0.3), (0.80, 0.64, 0.3, 0.3), (0.24, 0.85, 0.3, 0.3))\n",
    "\n",
    "TITLE_FS = 25\n",
    "LABEL_FS = 23\n",
    "TICK_FS = 21\n",
    "IMG_SIZE = 0.35\n",
    "\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Image: (336x336)', fontsize=TITLE_FS)\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.imshow(im0)\n",
    "ax2.set_title('DINOv2: (24x24)', fontsize=TITLE_FS)\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ot, om = 0.2, 150\n",
    "max_t = mem_time_results['Strided']['time'] + ot\n",
    "max_m = mem_time_results['LoftUp']['mem'] + om\n",
    "\n",
    "ax3.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax3.set_xlim(-ot, max_t)\n",
    "ax3.set_ylim(-om,  max_m)\n",
    "\n",
    "ax3.set_xlabel('Time (s)', fontsize=LABEL_FS)\n",
    "ax3.set_ylabel('Memory (MB)', fontsize=LABEL_FS)\n",
    "ax3.tick_params(axis='both', labelsize=TICK_FS)\n",
    "\n",
    "for i, res in enumerate(mem_time_results.items()):\n",
    "    im = imgs[i]\n",
    "    name, value = res\n",
    "    x = value['time']# / max_t\n",
    "    y = value['mem'] #/ max_m\n",
    "    ax3.scatter(x, y, lw=10)\n",
    "\n",
    "    o_img = OffsetImage(im, zoom=IMG_SIZE)\n",
    "    img_size = int(IMG_SIZE * 336)\n",
    "    loc = inset_locs[i]\n",
    "    ix, iy = loc[0] * max_t, loc[1] * max_m\n",
    "    ab = AnnotationBbox(o_img, (ix, iy), frameon=True, bboxprops=dict(edgecolor=f\"C{i}\", linewidth=8), pad=0)\n",
    "    ax3.add_artist(ab)\n",
    "ax3.legend(mem_time_results.keys(), fontsize=TICK_FS, loc='lower right')\n",
    "\n",
    "\n",
    "ax4.set_xlabel('Image length (px)', fontsize=LABEL_FS)\n",
    "ax4.set_ylabel('Time (s)', fontsize=LABEL_FS)\n",
    "ax4.tick_params(axis='both', labelsize=TICK_FS)\n",
    "\n",
    "for name, times in time_vs_l.items():\n",
    "    ax4.plot(lengths[:len(times)], times, label=name, marker='.', lw=3, ms=15)\n",
    "ax4.semilogy()\n",
    "ax4.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "ax5.set_xlabel('Image length (px)', fontsize=LABEL_FS)\n",
    "ax5.set_ylabel('Memory (MB)', fontsize=LABEL_FS)\n",
    "ax5.tick_params(axis='both', labelsize=TICK_FS)\n",
    "ax5.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "for name, mems in mem_vs_l.items():\n",
    "    ax5.plot(lengths[:len(mems)], mems, label=name, marker='.', lw=3, ms=15)\n",
    "ax5.semilogy()\n",
    "\n",
    "for key, ax in zip(('a', 'b', 'c'), (ax1, ax3, ax4)):\n",
    "    y = 1.016 if key =='b' else 1.05\n",
    "    x = -0.26 if key =='c' else -0.15\n",
    "    ax.text(x, y, f\"{key}.\", transform=ax.transAxes, \n",
    "            size=LABEL_FS + 4, weight='bold')\n",
    "\n",
    "plt.tight_layout(pad=2.5)\n",
    "plt.savefig('fig_out/perf_landscape.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
