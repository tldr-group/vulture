{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from time import time_ns\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from yoeo.main import get_dv2_model, get_upsampler_and_expr, get_hr_feats\n",
    "from yoeo.utils import load_image, to_numpy, closest_crop, Experiment, do_2D_pca\n",
    "\n",
    "from yoeo.comparisons.lift import ViTLiFTExtractor\n",
    "from yoeo.comparisons.strided import StridedDv2\n",
    "\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mem_time(\n",
    "    device: torch.device | str,\n",
    "    featurise_fn: Callable\n",
    ") -> tuple[float, float]:\n",
    "    torch.cuda.reset_peak_memory_stats(device)  # s.t memory is accurate\n",
    "    torch.cuda.synchronize(device)  # s.t time is accurate\n",
    "\n",
    "    def _to_MB(x: int) -> float:\n",
    "        return x / (1024**2)\n",
    "\n",
    "    def _to_s(t: int) -> float:\n",
    "        return t / 1e9\n",
    "\n",
    "    start_m = torch.cuda.max_memory_allocated(device)\n",
    "    start_t = time_ns()\n",
    "\n",
    "    featurise_fn()\n",
    "\n",
    "    end_m = torch.cuda.max_memory_allocated(device)\n",
    "    torch.cuda.synchronize(device)\n",
    "    end_t = time_ns()\n",
    "\n",
    "    return _to_MB(end_m - start_m), _to_s(end_t - start_t)\n",
    "\n",
    "def rescale(arr: np.ndarray, swap_channels: bool=True) -> np.ndarray:\n",
    "    if swap_channels:\n",
    "        arr = np.transpose(arr, (1, 2, 0))\n",
    "    h, w, c = arr.shape\n",
    "    flat = arr.reshape((h * w, c))\n",
    "    rescaled_flat = MinMaxScaler(clip=True).fit_transform(flat)\n",
    "    return rescaled_flat.reshape((h, w, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/ywyue_FiT3D_main\n"
     ]
    }
   ],
   "source": [
    "normal_dv2 = get_dv2_model(fit_3d=False, device=DEVICE)\n",
    "dv2 = get_dv2_model(True, device=DEVICE)\n",
    "\n",
    "model_path = \"../trained_models/e5000_full_fit_reg.pth\"\n",
    "cfg_path = \"../yoeo/models/configs/combined_no_shift.json\"\n",
    "\n",
    "upsampler, expr = get_upsampler_and_expr(model_path, cfg_path, device=DEVICE)\n",
    "# upsampler = upsampler.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "featup_jbu = torch.hub.load(\"mhamilton723/FeatUp\", \"dinov2\", use_norm=True).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "strided = StridedDv2('dinov2', 'vits14_reg', 1).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Backbone: dino_vits8\n",
      "Loaded LiFT module from: ../trained_models/lift/lift_dino_vits8.pth\n"
     ]
    }
   ],
   "source": [
    "lift_path = \"../trained_models/lift/lift_dino_vits8.pth\"\n",
    "lift = ViTLiFTExtractor('dino_vits8', lift_path=lift_path, channel=384, facet='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"fig_data/church_compare.png\"\n",
    "img = Image.open(PATH).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _our_featurise(img: Image.Image, dv2: torch.nn.Module, upsampler: torch.nn.Module, expr: Experiment) -> np.ndarray:\n",
    "    hr_feats = get_hr_feats(img, dv2, upsampler, DEVICE, n_ch_in=expr.n_ch_in)\n",
    "    hr_feats_np = to_numpy(hr_feats)\n",
    "    reduced_hr = hr_feats_np[:3]\n",
    "    return reduced_hr\n",
    "\n",
    "@torch.no_grad()\n",
    "def _jbu_featurise(path: str, jbu: torch.nn.Module) -> np.ndarray:\n",
    "    _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = _img.height, _img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    img, _ = load_image(path, tr)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        jbu_feats = jbu(img.to(torch.float32))\n",
    "    jbu_feats = F.interpolate(jbu_feats, (_h, _w))\n",
    "    jbu_feats_np = to_numpy(jbu_feats)\n",
    "    return jbu_feats_np[:3]\n",
    "\n",
    "@torch.no_grad()\n",
    "def _lift_featurise(path: str, lift: ViTLiFTExtractor, n: int=3, patch_size: int=8) -> np.ndarray:\n",
    "    _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = _img.height, _img.width\n",
    "\n",
    "    image_batch, _ = lift.preprocess(path, (_h, _w))\n",
    "    image_batch = image_batch.to(DEVICE)\n",
    "\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        lift_feats = lift.extract_descriptors_iterative_lift(image_batch, lift_iter=n)\n",
    "    _, _, c = lift_feats.shape\n",
    "    sf = int(patch_size / (2**n))\n",
    "    reshaped = lift_feats.squeeze(0).T.reshape((c, _h // sf, _w //  sf))\n",
    "    return to_numpy(reshaped)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _strided_featurise(path: str, strided: StridedDv2) -> np.ndarray:\n",
    "    _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = _img.height, _img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    img, _ = load_image(path, tr)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        strided_feats = strided(img)\n",
    "    strided_feats = F.interpolate(strided_feats, (_h, _w))\n",
    "    strided_feats_feats_np = to_numpy(strided_feats)\n",
    "    return strided_feats_feats_np\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_featurise = partial(_our_featurise, img=img, dv2=dv2, upsampler=upsampler, expr=expr)\n",
    "jbu_featurise = partial(_jbu_featurise, path=PATH, jbu=featup_jbu)\n",
    "lift_featurise = partial(_lift_featurise, path=PATH, lift=lift)\n",
    "strided_featurise = partial(_strided_featurise, path=PATH, strided=strided)\n",
    "\n",
    "fns = (our_featurise, jbu_featurise, lift_featurise, strided_featurise)\n",
    "featuriser_names = (\"Ours\", \"FeatUp (JBU)\", \"LiFT\", \"Strided\")\n",
    "mem_time_results: dict[str, dict] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry run\n",
    "for fn in fns:\n",
    "    mem, time = measure_mem_time(DEVICE, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ours': {'time': 0.09838406159999999, 'mem': 293.193359375}, 'FeatUp (JBU)': {'time': 0.13876388639999998, 'mem': 1038.5126953125}, 'LiFT': {'time': 0.0643809202, 'mem': 115.27278645833333}, 'Strided': {'time': 1.9693551552, 'mem': 371.07373046875}}\n"
     ]
    }
   ],
   "source": [
    "N_REPEATS =5\n",
    "for name, fn in zip(featuriser_names, fns):\n",
    "    mems, times = [], []\n",
    "    for i in range(N_REPEATS):\n",
    "        mem, time = measure_mem_time(DEVICE, fn)\n",
    "        if name in ('Strided', 'LiFT'):\n",
    "            # LiFT & strided upsample all 384 DINO features, JBU & Ours upsample to 128\n",
    "            # for fair comparison we scale memory of those approaches with that ratio\n",
    "            mem *= 128 / 384\n",
    "        mems.append(mem)\n",
    "        times.append(time)\n",
    "    mem_time_results[name] = {'time': np.mean(times), 'mem': np.mean(mems)}\n",
    "print(mem_time_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = rescale(our_featurise())\n",
    "im2 = rescale(jbu_featurise())\n",
    "lift_feats = lift_featurise()\n",
    "pcaed = do_2D_pca(lift_feats, 3)\n",
    "im3 = rescale(pcaed, swap_channels=False).astype(np.float32)\n",
    "stride_feats = strided_featurise()\n",
    "strided_pcaed = do_2D_pca(stride_feats, 3)\n",
    "im4 = rescale(strided_pcaed, swap_channels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "imgs = (im1, im2, im3, im4)\n",
    "fig, axs = plt.subplots(ncols=len(imgs))\n",
    "fig.set_size_inches((16, 10))\n",
    "for i in range(len(imgs)):\n",
    "    axs[i].imshow(imgs[i])\n",
    "    axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 2, 1], height_ratios=[1, 1])  \n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))  \n",
    "# Create subplots in specific grid locations\n",
    "ax1 = fig.add_subplot(gs[0, 0])  # Top-left\n",
    "ax2 = fig.add_subplot(gs[1, 0])  # Bottom-left\n",
    "ax3 = fig.add_subplot(gs[:, 1])  # Middle column (spanning both rows)\n",
    "ax4 = fig.add_subplot(gs[0, 2])  # Top-right\n",
    "ax5 = fig.add_subplot(gs[1, 2])  # Bottom-right\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "inset_locs = ((0.1, 0.42, 0.3, 0.3), (0.25, 0.82, 0.3, 0.3), (0.25, 0.05, 0.3, 0.3), (0.80, 0.5, 0.3, 0.3))\n",
    "\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Image: (336x336)', fontsize=20)\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.imshow(img)\n",
    "ax2.set_title('DINOv2 Features', fontsize=20)\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ot, om = 0.2, 150\n",
    "max_t = mem_time_results['Strided']['time'] + ot\n",
    "max_m = mem_time_results['FeatUp (JBU)']['mem'] + om\n",
    "\n",
    "ax3.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax3.set_xlim(-ot, max_t)\n",
    "ax3.set_ylim(-om,  max_m)\n",
    "\n",
    "ax3.set_xlabel('Time (s)', fontsize=18)\n",
    "ax3.set_ylabel('Memory (MB)', fontsize=18)\n",
    "ax3.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "for i, res in enumerate(mem_time_results.items()):\n",
    "    im = imgs[i]\n",
    "    name, value = res\n",
    "    x = value['time']# / max_t\n",
    "    y = value['mem'] #/ max_m\n",
    "    ax3.scatter(x, y, lw=10)\n",
    "\n",
    "    o_img = OffsetImage(im, zoom=0.45)\n",
    "    img_size = int(0.4 * 336)\n",
    "    loc = inset_locs[i]\n",
    "    ix, iy = loc[0] * max_t, loc[1] * max_m\n",
    "    ab = AnnotationBbox(o_img, (ix, iy), frameon=True, bboxprops=dict(edgecolor=f\"C{i}\", linewidth=8), pad=0)\n",
    "    ax3.add_artist(ab)\n",
    "ax3.legend(mem_time_results.keys(), fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
