{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f308c8ed030>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from time import time_ns\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "from yoeo.main import get_dv2_model, get_upsampler_and_expr, get_hr_feats\n",
    "from yoeo.utils import load_image, convert_image, to_numpy, closest_crop, Experiment, do_2D_pca, add_flash_attention\n",
    "\n",
    "from yoeo.comparisons.lift import ViTLiFTExtractor\n",
    "from yoeo.comparisons.strided import StridedDv2\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "SEED = 10672\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mem_time(\n",
    "    device: torch.device | str,\n",
    "    featurise_fn: Callable\n",
    ") -> tuple[float, float]:\n",
    "    torch.cuda.reset_peak_memory_stats(device)  # s.t memory is accurate\n",
    "    torch.cuda.synchronize(device)  # s.t time is accurate\n",
    "\n",
    "    def _to_MB(x: int) -> float:\n",
    "        return x / (1024**2)\n",
    "\n",
    "    def _to_s(t: int) -> float:\n",
    "        return t / 1e9\n",
    "\n",
    "    start_m = torch.cuda.max_memory_allocated(device)\n",
    "    start_t = time_ns()\n",
    "\n",
    "    featurise_fn()\n",
    "\n",
    "    end_m = torch.cuda.max_memory_allocated(device)\n",
    "    torch.cuda.synchronize(device)\n",
    "    end_t = time_ns()\n",
    "\n",
    "    return _to_MB(end_m - start_m), _to_s(end_t - start_t)\n",
    "\n",
    "def rescale(arr: np.ndarray, swap_channels: bool=True) -> np.ndarray:\n",
    "    if swap_channels:\n",
    "        arr = np.transpose(arr, (1, 2, 0))\n",
    "    h, w, c = arr.shape\n",
    "    flat = arr.reshape((h * w, c))\n",
    "    rescaled_flat = MinMaxScaler(clip=True).fit_transform(flat)\n",
    "    return rescaled_flat.reshape((h, w, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/ywyue_FiT3D_main\n"
     ]
    }
   ],
   "source": [
    "normal_dv2 = get_dv2_model(fit_3d=False, device=DEVICE)\n",
    "dv2 = get_dv2_model(True, device=DEVICE)\n",
    "\n",
    "model_path = \"../trained_models/e5000_full_fit_reg.pth\"\n",
    "cfg_path = \"../yoeo/models/configs/combined_no_shift.json\"\n",
    "\n",
    "upsampler, expr = get_upsampler_and_expr(model_path, cfg_path, device=DEVICE)\n",
    "# upsampler = upsampler.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "featup_jbu = torch.hub.load(\"mhamilton723/FeatUp\", \"dinov2\", use_norm=True).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "strided = StridedDv2('dinov2', 'vits14_reg', 1).to(DEVICE).eval()\n",
    "strided.model = add_flash_attention(strided.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Backbone: dino_vits8\n",
      "Loaded LiFT module from: ../trained_models/lift/lift_dino_vits8.pth\n"
     ]
    }
   ],
   "source": [
    "lift_path = \"../trained_models/lift/lift_dino_vits8.pth\"\n",
    "lift = ViTLiFTExtractor('dino_vits8', lift_path=lift_path, channel=384, facet='key')\n",
    "# lift.extractor.model = add_flash_attention(lift.extractor.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"fig_data/perf_landscape/church_compare.png\"\n",
    "img = Image.open(PATH).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _our_featurise(img: Image.Image, dv2: torch.nn.Module, upsampler: torch.nn.Module, expr: Experiment) -> np.ndarray:\n",
    "    hr_feats = get_hr_feats(img, dv2, upsampler, DEVICE, n_ch_in=expr.n_ch_in)\n",
    "    hr_feats_np = to_numpy(hr_feats)\n",
    "    reduced_hr = hr_feats_np[:3]\n",
    "    return reduced_hr\n",
    "\n",
    "@torch.no_grad()\n",
    "def _original_featurise(img: Image.Image, dv2: torch.nn.Module) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr)\n",
    "    _, _, h, w = tensor.shape\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        dino_feats = dv2.forward_features(tensor)['x_norm_patchtokens']\n",
    "    n_patch_w, n_patch_h = w // 14, h // 14\n",
    "    dino_feats = dino_feats.permute((0, 2, 1))\n",
    "    dino_feats = dino_feats.reshape((1, -1, n_patch_h, n_patch_w,))\n",
    "    dino_feats_np = to_numpy(dino_feats)\n",
    "    return dino_feats_np\n",
    "\n",
    "@torch.no_grad()\n",
    "def _jbu_featurise(img: Image.Image, jbu: torch.nn.Module) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        jbu_feats = jbu(tensor.to(torch.float32))\n",
    "    jbu_feats = F.interpolate(jbu_feats, (_h, _w))\n",
    "    jbu_feats_np = to_numpy(jbu_feats)\n",
    "    return jbu_feats_np[:3]\n",
    "\n",
    "@torch.no_grad()\n",
    "def _lift_featurise(path: str, img: Image.Image, lift: ViTLiFTExtractor, n: int=3, patch_size: int=8) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "\n",
    "    image_batch, _ = lift.preprocess(path, (_h, _w))\n",
    "    image_batch = image_batch.to(DEVICE)\n",
    "\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        lift_feats = lift.extract_descriptors_iterative_lift(image_batch, lift_iter=n)\n",
    "    _, _, c = lift_feats.shape\n",
    "    sf = int(patch_size / (2**n))\n",
    "    reshaped = lift_feats.squeeze(0).T.reshape((c, _h // sf, _w //  sf))\n",
    "    return to_numpy(reshaped)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _strided_featurise(img: Image.Image, strided: StridedDv2) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        strided_feats = strided(tensor)\n",
    "    strided_feats = F.interpolate(strided_feats, (_h, _w))\n",
    "    strided_feats_feats_np = to_numpy(strided_feats)\n",
    "    return strided_feats_feats_np\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_featurise = partial(_our_featurise, img=img, dv2=dv2, upsampler=upsampler, expr=expr)\n",
    "original_featurise = partial(_original_featurise, img=img, dv2=normal_dv2)\n",
    "jbu_featurise = partial(_jbu_featurise, img=img, jbu=featup_jbu)\n",
    "lift_featurise = partial(_lift_featurise, path=PATH, img=img, lift=lift)\n",
    "strided_featurise = partial(_strided_featurise, img=img, strided=strided)\n",
    "\n",
    "fns = (our_featurise, jbu_featurise, lift_featurise, strided_featurise)\n",
    "featuriser_names = (\"Ours\", \"FeatUp (JBU)\", \"LiFT\", \"Strided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry run\n",
    "for fn in fns:\n",
    "    mem, time = measure_mem_time(DEVICE, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ours': {'time': 0.0975382752, 'mem': 293.193359375}, 'FeatUp (JBU)': {'time': 0.1310169948, 'mem': 1038.5126953125}, 'LiFT': {'time': 0.061607448200000005, 'mem': 115.27278645833333}, 'Strided': {'time': 2.1706915068, 'mem': 371.07373046875}}\n"
     ]
    }
   ],
   "source": [
    "mem_time_results: dict[str, dict] = {}\n",
    "N_REPEATS =5\n",
    "for name, fn in zip(featuriser_names, fns):\n",
    "    mems, times = [], []\n",
    "    for i in range(N_REPEATS):\n",
    "        mem, time = measure_mem_time(DEVICE, fn)\n",
    "        if name in ('Strided', 'LiFT'):\n",
    "            # LiFT & strided upsample all 384 DINO features, JBU & Ours upsample to 128\n",
    "            # for fair comparison we scale memory of those approaches with that ratio\n",
    "            mem *= 128 / 384\n",
    "        mems.append(mem)\n",
    "        times.append(time)\n",
    "    mem_time_results[name] = {'time': np.mean(times), 'mem': np.mean(mems)}\n",
    "print(mem_time_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ours': [130.697265625, 210.31103515625, 314.50341796875, 430.13525390625, 580.919921875, 727.86669921875, 926.1728515625, 1153.587890625, 1353.32470703125, 1626.40283203125], 'FeatUp (JBU)': [483.38427734375, 731.4267578125, 1122.95947265625, 1489.2509765625, 2036.0791015625, 2532.7001953125, 3222.9599609375, 4001.646484375, 4691.2041015625, 5627.0888671875], 'LiFT': [58.58740234375, 89.16471354166666, 125.30289713541666, 168.88297526041666, 218.64127604166666, 276.2392578125, 405.64225260416663, 595.2775065104166, 847.32568359375, 1174.4586588541665], 'Strided': [166.4130859375, 257.95914713541663, 402.5550130208333, 542.3623046875, 747.2275390625, 935.1884765625, 1200.6023763020833, 1499.5504557291665, 1762.6240234375, 2122.0133463541665]}\n",
      "{'Ours': [0.083556023, 0.099365946, 0.129589985, 0.162557875, 0.214227614, 0.265708625, 0.332228743, 0.402966352, 0.490823776, 0.600757946], 'FeatUp (JBU)': [0.072197202, 0.103229142, 0.150156703, 0.20095606, 0.268559712, 0.387262323, 0.426323897, 0.615717608, 0.698517641, 0.836370735], 'LiFT': [0.049683964, 0.058205537, 0.072551352, 0.100490855, 0.140858772, 0.190125485, 0.250319679, 0.326687811, 0.421088714, 0.515644151], 'Strided': [0.42400825, 1.035338077, 2.604720957, 4.953524735, 9.823992254, 15.906710145, 27.027960576, 42.932990226, 59.432803736, 86.656496629]}\n"
     ]
    }
   ],
   "source": [
    "lengths = list(range(224, 896 - 64, 64))\n",
    "\n",
    "our_featurise_l = partial(_our_featurise, dv2=dv2, upsampler=upsampler, expr=expr)\n",
    "jbu_featurise_l = partial(_jbu_featurise, jbu=featup_jbu)\n",
    "lift_featurise_l = partial(_lift_featurise, path=PATH, lift=lift)\n",
    "strided_featurise_l = partial(_strided_featurise, strided=strided)\n",
    "\n",
    "fns_vs_l = (our_featurise_l, jbu_featurise_l, lift_featurise_l, strided_featurise_l)\n",
    "\n",
    "mem_vs_l: dict[str, list[float]] = {}\n",
    "time_vs_l: dict[str, list[float]] = {}\n",
    "\n",
    "for name, fn in zip(featuriser_names, fns_vs_l):\n",
    "    mems, times = [], []\n",
    "    for l in lengths:\n",
    "        resized = img.resize((l, l))\n",
    "        bound = partial(fn, img=resized)\n",
    "        mem, time = measure_mem_time(DEVICE, bound)\n",
    "        if name in ('Strided', 'LiFT'):\n",
    "\n",
    "            mem *= 128 / 384\n",
    "        mems.append(mem)\n",
    "        times.append(time)\n",
    "    mem_vs_l[name] = mems\n",
    "    time_vs_l[name] = times\n",
    "\n",
    "print(mem_vs_l)\n",
    "print(time_vs_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 24, 3)\n"
     ]
    }
   ],
   "source": [
    "original_pcaed = do_2D_pca(original_featurise(), 3)\n",
    "im0 = rescale(original_pcaed, swap_channels=False)\n",
    "print(im0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = rescale(our_featurise())\n",
    "im2 = rescale(jbu_featurise())\n",
    "lift_feats = lift_featurise()\n",
    "pcaed = do_2D_pca(lift_feats, 3)\n",
    "im3 = rescale(pcaed, swap_channels=False).astype(np.float32)\n",
    "stride_feats = strided_featurise()\n",
    "strided_pcaed = do_2D_pca(stride_feats, 3)\n",
    "im4 = rescale(strided_pcaed, swap_channels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "imgs = (im1, im2, im3, im4)\n",
    "fig, axs = plt.subplots(ncols=len(imgs))\n",
    "fig.set_size_inches((16, 10))\n",
    "for i in range(len(imgs)):\n",
    "    axs[i].imshow(imgs[i])\n",
    "    axs[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 2, 1], height_ratios=[1, 1])  \n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))  \n",
    "# Create subplots in specific grid locations\n",
    "ax1 = fig.add_subplot(gs[0, 0])  # Top-left\n",
    "ax2 = fig.add_subplot(gs[1, 0])  # Bottom-left\n",
    "ax3 = fig.add_subplot(gs[:, 1])  # Middle column (spanning both rows)\n",
    "ax4 = fig.add_subplot(gs[0, 2])  # Top-right\n",
    "ax5 = fig.add_subplot(gs[1, 2])  # Bottom-right\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "inset_locs = ((0.12, 0.44, 0.3, 0.3), (0.27, 0.82, 0.3, 0.3), (0.25, 0.05, 0.3, 0.3), (0.80, 0.5, 0.3, 0.3))\n",
    "\n",
    "TITLE_FS = 25\n",
    "LABEL_FS = 23\n",
    "TICK_FS = 21\n",
    "\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Image: (336x336)', fontsize=TITLE_FS)\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.imshow(im0)\n",
    "ax2.set_title('DINOv2: (24x24)', fontsize=TITLE_FS)\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ot, om = 0.2, 150\n",
    "max_t = mem_time_results['Strided']['time'] + ot\n",
    "max_m = mem_time_results['FeatUp (JBU)']['mem'] + om\n",
    "\n",
    "ax3.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax3.set_xlim(-ot, max_t)\n",
    "ax3.set_ylim(-om,  max_m)\n",
    "\n",
    "ax3.set_xlabel('Time (s)', fontsize=LABEL_FS)\n",
    "ax3.set_ylabel('Memory (MB)', fontsize=LABEL_FS)\n",
    "ax3.tick_params(axis='both', labelsize=TICK_FS)\n",
    "\n",
    "for i, res in enumerate(mem_time_results.items()):\n",
    "    im = imgs[i]\n",
    "    name, value = res\n",
    "    x = value['time']# / max_t\n",
    "    y = value['mem'] #/ max_m\n",
    "    ax3.scatter(x, y, lw=10)\n",
    "\n",
    "    o_img = OffsetImage(im, zoom=0.48)\n",
    "    img_size = int(0.4 * 336)\n",
    "    loc = inset_locs[i]\n",
    "    ix, iy = loc[0] * max_t, loc[1] * max_m\n",
    "    ab = AnnotationBbox(o_img, (ix, iy), frameon=True, bboxprops=dict(edgecolor=f\"C{i}\", linewidth=8), pad=0)\n",
    "    ax3.add_artist(ab)\n",
    "ax3.legend(mem_time_results.keys(), fontsize=TICK_FS)\n",
    "\n",
    "\n",
    "ax4.set_xlabel('Image length (px)', fontsize=LABEL_FS)\n",
    "ax4.set_ylabel('Time (s)', fontsize=LABEL_FS)\n",
    "ax4.tick_params(axis='both', labelsize=TICK_FS)\n",
    "\n",
    "for name, times in time_vs_l.items():\n",
    "    ax4.plot(lengths, times, label=name, marker='.', lw=3, ms=15)\n",
    "ax4.semilogy()\n",
    "ax4.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "ax5.set_xlabel('Image length (px)', fontsize=LABEL_FS)\n",
    "ax5.set_ylabel('Memory (MB)', fontsize=LABEL_FS)\n",
    "ax5.tick_params(axis='both', labelsize=TICK_FS)\n",
    "ax5.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "for name, mems in mem_vs_l.items():\n",
    "    ax5.plot(lengths, mems, label=name, marker='.', lw=3, ms=15)\n",
    "ax5.semilogy()\n",
    "\n",
    "for key, ax in zip(('a', 'b', 'c'), (ax1, ax3, ax4)):\n",
    "    y = 1.016 if key =='b' else 1.05\n",
    "    x = -0.26 if key =='c' else -0.15\n",
    "    ax.text(x, y, f\"{key}.\", transform=ax.transAxes, \n",
    "            size=LABEL_FS + 4, weight='bold')\n",
    "\n",
    "plt.tight_layout(pad=2.5)\n",
    "plt.savefig('fig_out/perf_landscape.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
