{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4565ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N CPUS: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronan/miniconda3/envs/dv2/lib/python3.12/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from interactive_seg_backend.file_handling import load_image, load_labels\n",
    "from interactive_seg_backend.main import featurise, train_and_apply, TrainingConfig, FeatureConfig\n",
    "\n",
    "from yoeo.main import (\n",
    "    get_hr_feats,\n",
    "    get_dv2_model,\n",
    "    get_upsampler_and_expr,\n",
    ")\n",
    "from yoeo.utils import convert_image, to_numpy, closest_crop, Experiment, do_2D_pca, add_flash_attention\n",
    "from yoeo.comparisons.lift import ViTLiFTExtractor\n",
    "from yoeo.comparisons.strided import StridedDv2\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.color import label2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Any, Literal\n",
    "\n",
    "SEED = 10673\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda:1\"\n",
    "\n",
    "FeatureTypes = Literal[\"classical\", \"dv2_nearest\", \"dv2_bilinear\", \"strided\", \"featup_jbu\", \"lift\", \"hr_vit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e113e",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- test ni supealloy/cracks (i.e small tertiary phases)\n",
    "    - cracks is good: it's where we need to align the notions of particleness from deep feats with classical feats \n",
    "- overlay onto image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4bb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/ywyue_FiT3D_main\n"
     ]
    }
   ],
   "source": [
    "normal_dv2 = get_dv2_model(fit_3d=False, device=DEVICE)\n",
    "dv2 = get_dv2_model(True, device=DEVICE)\n",
    "\n",
    "model_path = \"../trained_models/e5000_full_fit_reg.pth\"\n",
    "cfg_path = \"../yoeo/models/configs/combined_no_shift.json\"\n",
    "\n",
    "upsampler, expr = get_upsampler_and_expr(model_path, cfg_path, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce8281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "featup_jbu = torch.hub.load(\"mhamilton723/FeatUp\", \"dinov2\", use_norm=True).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e532420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "strided = StridedDv2('dinov2', 'vits14_reg', 1).to(DEVICE).eval()\n",
    "strided.model = add_flash_attention(strided.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39ff437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ronan/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Backbone: dino_vits8\n",
      "Loaded LiFT module from: ../trained_models/lift/lift_dino_vits8.pth\n"
     ]
    }
   ],
   "source": [
    "lift_path = \"../trained_models/lift/lift_dino_vits8.pth\"\n",
    "lift = ViTLiFTExtractor('dino_vits8', lift_path=lift_path, channel=384, facet='key', device=DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6332f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _our_featurise(img: Image.Image, dv2: torch.nn.Module, upsampler: torch.nn.Module, n_ch_in: int) -> np.ndarray:\n",
    "    hr_feats = get_hr_feats(img, dv2, upsampler, DEVICE, n_ch_in=n_ch_in)\n",
    "    hr_feats_np = to_numpy(hr_feats)\n",
    "    reduced_hr = hr_feats_np\n",
    "    return reduced_hr\n",
    "\n",
    "@torch.no_grad()\n",
    "def _original_featurise(img: Image.Image, dv2: torch.nn.Module, resize: str | None=\"nearest\") -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr, device_str=DEVICE)\n",
    "    _, _, h, w = tensor.shape\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        dino_feats = dv2.forward_features(tensor)['x_norm_patchtokens']\n",
    "    n_patch_w, n_patch_h = w // 14, h // 14\n",
    "    dino_feats = dino_feats.permute((0, 2, 1))\n",
    "    dino_feats = dino_feats.reshape((1, -1, n_patch_h, n_patch_w,))\n",
    "    if resize == \"nearest\":\n",
    "        dino_feats = F.interpolate(dino_feats, (_h, _w), mode='nearest')\n",
    "    elif resize == \"bilinear\":\n",
    "        dino_feats = F.interpolate(dino_feats, (_h, _w), mode='bilinear')\n",
    "    dino_feats_np = to_numpy(dino_feats)\n",
    "    return dino_feats_np\n",
    "\n",
    "@torch.no_grad()\n",
    "def _jbu_featurise(img: Image.Image, jbu: torch.nn.Module) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr, device_str=DEVICE)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        jbu_feats = jbu(tensor.to(torch.float32))\n",
    "    jbu_feats = F.interpolate(jbu_feats, (_h, _w))\n",
    "    jbu_feats_np = to_numpy(jbu_feats)\n",
    "    return jbu_feats_np\n",
    "\n",
    "@torch.no_grad()\n",
    "def _lift_featurise(path: str, img: Image.Image, lift: ViTLiFTExtractor, n: int=3, patch_size: int=8) -> np.ndarray:\n",
    "    def closest_mult(x: int, p: int=8) -> int:\n",
    "        return x - (x % p)\n",
    "    tr_h, tr_w = closest_mult(img.height, 8), closest_mult(img.width, 8)\n",
    "    _h, _w = img.height, img.width\n",
    "\n",
    "    image_batch, _ = lift.preprocess(path, (tr_h, tr_w))\n",
    "    image_batch = image_batch.to(DEVICE)\n",
    "\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        lift_feats = lift.extract_descriptors_iterative_lift(image_batch, lift_iter=n)\n",
    "    _, _, c = lift_feats.shape\n",
    "    sf = int(patch_size / (2**n))\n",
    "    reshaped = lift_feats.squeeze(0).T.reshape((1, c, tr_h // sf, tr_w //  sf))\n",
    "    print(reshaped.shape)\n",
    "    resized = F.interpolate(reshaped, (_h, _w))\n",
    "    return to_numpy(resized)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _strided_featurise(img: Image.Image, strided: StridedDv2) -> np.ndarray:\n",
    "    # _img = Image.open(path).convert(\"RGB\")\n",
    "    _h, _w = img.height, img.width\n",
    "    tr = closest_crop(_h, _w)\n",
    "\n",
    "    tensor = convert_image(img, tr, device_str=DEVICE)\n",
    "    with torch.autocast(\"cuda\", torch.float16):\n",
    "        strided_feats = strided(tensor)\n",
    "    strided_feats = F.interpolate(strided_feats, (_h, _w))\n",
    "    strided_feats_feats_np = to_numpy(strided_feats)\n",
    "    return strided_feats_feats_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e7217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"fig_data/supp_upsampler_choice\"\n",
    "tc = TrainingConfig(FeatureConfig(), classifier='xgb', classifier_params={'class_weight': 'balanced'})\n",
    "# img_path = f\"{PATH}/img_patch14.tif\"\n",
    "img_path = f\"{PATH}/noisy_NMC_cracks.png\"\n",
    "img = Image.fromarray(load_image(f\"{img_path}\")).convert('RGB')\n",
    "# labels = load_labels(f\"{PATH}/labels_patch14_more.tiff\")\n",
    "labels = load_labels(f\"{PATH}/noisy_nmc_labels_more.tiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56086ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(img: Image.Image, feat_type: FeatureTypes) -> tuple[np.ndarray, np.ndarray]:\n",
    "    classical_feats = featurise(np.array(img), tc)\n",
    "    if feat_type == \"classical\":\n",
    "        return (classical_feats, classical_feats)\n",
    "    elif feat_type == \"dv2_nearest\":\n",
    "        deep_feats = _original_featurise(img, normal_dv2, 'nearest')\n",
    "    elif feat_type == \"dv2_bilinear\":\n",
    "        deep_feats = _original_featurise(img, normal_dv2, 'bilinear')\n",
    "    elif feat_type == \"strided\":\n",
    "        deep_feats = _strided_featurise(img, strided)\n",
    "    elif feat_type == \"featup_jbu\":\n",
    "        deep_feats = _jbu_featurise(img, featup_jbu)\n",
    "    elif feat_type == \"lift\":\n",
    "        deep_feats = _lift_featurise(img_path, img, lift)\n",
    "    elif feat_type == \"hr_vit\":\n",
    "        deep_feats = _our_featurise(img, dv2, upsampler, expr.n_ch_in)\n",
    "    \n",
    "    deep_feats = np.transpose(deep_feats, (1, 2, 0))\n",
    "    return (classical_feats, deep_feats)\n",
    "\n",
    "def rescale(arr: np.ndarray, swap_channels: bool=True) -> np.ndarray:\n",
    "    if swap_channels:\n",
    "        arr = np.transpose(arr, (1, 2, 0))\n",
    "    h, w, c = arr.shape\n",
    "    flat = arr.reshape((h * w, c))\n",
    "    rescaled_flat = minmax_scale(flat)\n",
    "    return rescaled_flat.reshape((h, w, c))\n",
    "\n",
    "def get_feat_vis(feats: np.ndarray, feat_type: FeatureTypes) -> np.ndarray:\n",
    "    if feat_type in (\"hr_vit\", ):\n",
    "        return rescale(feats[:, :, :3], swap_channels=False)\n",
    "    else:\n",
    "        tmp_transpose = feats.transpose((2, 0, 1))\n",
    "        reduced = do_2D_pca(tmp_transpose, 3)\n",
    "        reshaped = reduced.transpose((0, 1, 2))\n",
    "        return rescale(reshaped, swap_channels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6118fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classical\n",
      "dv2_nearest\n",
      "dv2_bilinear\n",
      "strided\n",
      "featup_jbu\n",
      "lift\n",
      "torch.Size([1, 384, 576, 680])\n",
      "hr_vit\n"
     ]
    }
   ],
   "source": [
    "features = (\"classical\", \"dv2_nearest\", \"dv2_bilinear\", \"strided\", \"featup_jbu\", \"lift\", \"hr_vit\")\n",
    "res: dict[FeatureTypes, dict] = {}\n",
    "\n",
    "for feat_type in features:\n",
    "    print(feat_type)\n",
    "    res[feat_type] = {}\n",
    "    classical, deep = get_features(img, feat_type)\n",
    "    feats = np.concatenate((classical, deep), -1)\n",
    "    # feats = deep\n",
    "    res[feat_type][\"feat_vis\"] = get_feat_vis(deep, feat_type)\n",
    "\n",
    "    pred, _, _ = train_and_apply(feats, labels, tc)\n",
    "    res[feat_type][\"pred\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d4b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_list = [[255, 255, 255], [31, 119, 180], [255, 127, 14], [44, 160, 44], [255, 0, 0]]\n",
    "color_list = [[255, 255, 255], [0, 62, 131], [181, 209, 204], [250, 43, 0], [255, 184, 82]]\n",
    "COLORS = np.array(color_list) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71159adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_labels_as_overlay(labels: np.ndarray, img: Image.Image, colors: list, alpha: float=1.0) -> Image.Image:\n",
    "    labels_unsqueezed = np.expand_dims(labels, -1)\n",
    "\n",
    "    overlay = label2rgb(labels, colors=colors[1:], kind='overlay', bg_label=0, image_alpha=1, alpha=alpha)\n",
    "    out = np.where(labels_unsqueezed, overlay * 255, np.array(img)).astype(np.uint8)\n",
    "    img_with_labels = Image.fromarray(out)\n",
    "    return img_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eef7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_inset_zoom(xywh: list[int], fig_xywh: list[float], img_arr: np.ndarray, labels: np.ndarray | None, ax ) -> object:\n",
    "    x0, y0, w, h = xywh\n",
    "    H, W, C = img_arr.shape\n",
    "    inset_data = np.zeros_like(img_arr)\n",
    "    inset_data[y0:y0+h, x0:x0+w, :] = img_arr[y0:y0+h, x0:x0+w, :]\n",
    "\n",
    "    axin = ax.inset_axes(\n",
    "        fig_xywh, xlim=(x0, x0+w), ylim=(y0, y0+h))\n",
    "    axin.set_xticks([])\n",
    "    axin.set_yticks([])\n",
    "    #axin.set_axis_off()\n",
    "    if labels is not None:\n",
    "        inset_data = label2rgb(labels, img_arr, COLORS[1:], kind='overlay', alpha=1, bg_label=-1)\n",
    "        axin.imshow(inset_data,)\n",
    "    else:\n",
    "        axin.imshow(inset_data, cmap=\"binary_r\",) # cmap=\"binary_r\"\n",
    "    ax.indicate_inset_zoom(axin, edgecolor=\"black\", lw=2)\n",
    "    axin.set_ylim((y0 + h, y0))\n",
    "\n",
    "    axin.patch.set_edgecolor('black')  \n",
    "\n",
    "    axin.patch.set_linewidth(4)  \n",
    "\n",
    "    return axin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "144b3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "TITLE_FS = 25\n",
    "LABEL_FS = 23\n",
    "TICK_FS = 21\n",
    "\n",
    "width = 4\n",
    "height = 4\n",
    "N = len(features) \n",
    "#figsize=(width * 2 * 2, height * (N // 2))\n",
    "\n",
    "half_N = (N // 2)\n",
    "nrows = 1 + half_N\n",
    "ncols = 4\n",
    "fig = plt.figure(figsize=(width * ncols, height * nrows ))\n",
    "gs = GridSpec(nrows, ncols, figure=fig)\n",
    "\n",
    "\n",
    "zoom_1_data_loc, zoom_2_data_loc = [140, 310, 90, 90], [140, 20, 120, 120]\n",
    "zoom_1_fig_loc, zoom_2_fig_loc = [-0.2, 0.0, 0.3, 0.3], [0.8, 0.6, 0.4, 0.4]\n",
    "\n",
    "\n",
    "ax_top = fig.add_subplot(gs[0, :2])  # Span across both column\n",
    "img_with_labels = apply_labels_as_overlay(labels, img, colors=COLORS)\n",
    "add_inset_zoom(zoom_1_data_loc, zoom_1_fig_loc, np.array(img_with_labels), None, ax=ax_top)\n",
    "add_inset_zoom(zoom_2_data_loc, zoom_2_fig_loc, np.array(img_with_labels), None, ax=ax_top)\n",
    "\n",
    "ax_top.set_title(\"Image + labels\", fontsize=TITLE_FS)\n",
    "ax_top.imshow(img_with_labels)\n",
    "ax_top.set_axis_off()\n",
    "\n",
    "titles = [\"Classical\", \"+Dv2 nearest\", \"+Dv2 bilinear\", \"+Strided\", \"+Featup (JBU)\", \"+LiFT\", \"+HR ViT\"]\n",
    "for i, (feat, subdict) in enumerate(res.items()):\n",
    "    # if feat in (\"strided\", \"lift\", \"featup_jbu\"):\n",
    "    #     continue\n",
    "    if feat == \"classical\":\n",
    "        row, col_1, col_2 = 0, 2, 3\n",
    "    else:\n",
    "        row =  (i - 1) % half_N + 1\n",
    "        col_1 = 2 * ((i - 1) // half_N)\n",
    "        col_2 = 2 * ((i - 1) // half_N) + 1\n",
    "\n",
    "    feat_ax = fig.add_subplot(gs[row, col_1])\n",
    "    feat_ax.set_title(titles[i], fontsize=TITLE_FS)\n",
    "    feat_ax.imshow(subdict[\"feat_vis\"])\n",
    "    pred_ax = fig.add_subplot(gs[row, col_2])\n",
    "\n",
    "    pred_recoloured = label2rgb(subdict[\"pred\"] + 1, colors=COLORS[1:])\n",
    "    pred_ax.imshow(pred_recoloured)\n",
    "\n",
    "    add_inset_zoom(zoom_1_data_loc, zoom_1_fig_loc, pred_recoloured, None, ax=pred_ax)\n",
    "    add_inset_zoom(zoom_2_data_loc, zoom_2_fig_loc, pred_recoloured, None, ax=pred_ax)\n",
    "\n",
    "    for ax in (feat_ax, pred_ax):\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_out/supp_upsampler_choice.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
